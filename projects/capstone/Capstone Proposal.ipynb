{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree - Capstone Proposal\n",
    "Yannis Pappas   \n",
    "October 16tt, 2018\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "Crime is a social phenomenon as old as societies themselves; and although there will never be a free from crime society, just because it would need everyone in that society to think and act in the same way, societies always look for a way to minimize it and prevent it.  \n",
    "\n",
    "In the modern United States history, crime rates increased after World War II,  peaking from the 1970s to the early 1990s. Violent crime nearly quadrupled between 1960 and its peak in 1991. Property crime more than doubled over the same period. Since the 1990s, however, crime in the United States has declined steadily.  \n",
    "\n",
    "Until recently crime prevention was studied based on strict behavioral and social methods, but the recent developments in Data Analysis have allowed a more quantitative approach in the subject.  \n",
    "\n",
    "**Predictive policing** is a domain of Criminology that refers to the use of mathematical, predictive and analytical techniques in law enforcement to identify potential criminal activity. Predictive policing methods fall into four general categories:  \n",
    "* methods for predicting crimes,  \n",
    "* methods for predicting offenders,  \n",
    "* methods for predicting perpetrators' identities,  \n",
    "* and methods for predicting victims of crime.  \n",
    "\n",
    ">In a future society, three mutants foresee all crime before it occurs. Plugged into a great machine, these \"precogs\" allow the Precrime Division to arrest suspects before any infliction of public harm.  \n",
    "\n",
    "Although we are far from Philip K. Dick's vision of crime prevention, we will try to evaluate the possibility of a particular crime will take place in a specific location and time based on past data.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.  \n",
    "\n",
    "Today, the city is known more for its tech scene than its criminal past. However, with rising wealth inequality, housing shortages, and the proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.\n",
    "\n",
    "Provided a dataset of nearly 12 years of crime reports from across all of San Francisco's neighborhoods we will create a model to predict the category of crime that occurred, given the time and location.\n",
    "\n",
    "## Datasets and Inputs\n",
    "\n",
    "The dataset contains incidents derived from the SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015.  \n",
    "\n",
    "* **Dates** - timestamp of the crime incident.\n",
    "* **Category** - category of the crime incident (only in train.csv). \n",
    "* **Descript** - detailed description of the crime incident (only in train.csv)\n",
    "* **DayOfWeek** - the day of the week\n",
    "* **PdDistrict** - name of the Police Department District\n",
    "* **Resolution** - how the crime incident was resolved (only in train.csv)\n",
    "* **Address** - the approximate street address of the crime incident \n",
    "* **X** - Longitude\n",
    "* **Y** - Latitude\n",
    "\n",
    "From the above variables **Descript** is the target variable we are going to predict (dependent variable).\n",
    "\n",
    "Since this is a (past) Kaggle competition, there are two datasets provided. The 'training set' which is labeled with the true crime description and a 'test set' which can be used for scoring and evaluation by Kaggle. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to the training set. \n",
    "\n",
    "## Solution Statement\n",
    "\n",
    "Our target will be to predict the category of crime that occurred, given the time and location. To address this, after the needed preprocessing we will evaluate a number of algorithms starting with simple algorithms like Multiclass Logistic Regression, then we will progress to more complex algorithms like Support Vector Machines and then to Ensemble Classifiers. Finally, We will try to address the problem with the use of Neural Networks.\n",
    "\n",
    "For a more thorough description of the process that will be followed, please see the Project Design section below.\n",
    "\n",
    "## Benchmark Model\n",
    "\n",
    "Several approaches have been applied to the specific problem with the most popular being:\n",
    "* XGBoost implementation of the Gradient Boosting algorithm (logloss = 2.29196),\n",
    "* Random Forests (logloss = 2.36808), and\n",
    "* Logistic Regression (logloss = 2.50732)  \n",
    "\n",
    "The ultimate benchmark will be the model created by Kaggle's user @voltron1985 (no further information about his model) which achieved 1.95936 score.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "The evaluation metric will be the multi-class logarithmic loss. Logarithmic loss measures the performance of a classification model where the prediction output is a probability value between 0 and 1. \n",
    "For each incident, we will predict a set of predicted probabilities (one for every class). The formula is then,\n",
    "\n",
    "\\begin{equation}\n",
    "logloss = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M}y_{ij}log(p_{ij})\n",
    "\\end{equation}  \n",
    "\n",
    "where $N$ is the number of cases in the test set, $M$ is the number of class labels, $log$ is the natural logarithm, $y_{ij}$ is 1 if observation $i$ is in class $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that observation $i$ belongs to class $j$.  \n",
    "\n",
    "\n",
    "## Project Design\n",
    "\n",
    "We will follow the following workflow during the project:\n",
    "\n",
    "![workflow](./workflow.png)\n",
    "\n",
    "### Data Wrangling\n",
    "* **Audit the quality of the data**  \n",
    "  *Audit data types* - Data Types should be DateTime for the 'Dates' field,  string for 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution' and 'Address', and float for 'X' and 'Y'  \n",
    "  *Audit missing values* - All fields should contain values otherwise they will be imputed or removed.  \n",
    "  *Audit values' ranges* - The numerical values will be checked for values between valid chronological and geographical boundaries.  \n",
    "  *Audit unique data points* - There should be no duplicates in the dataset.  \n",
    "  *Audit a Cross-Field Constraint* - The 'DayOfWeek' should much the 'Dates'.  \n",
    "        \n",
    "* **Create a Data Cleaning Plan**\n",
    "    * Identify the causes of errors.\n",
    "    * Define operations that will address these errors\n",
    "    * Test that these operations address the quality problems\n",
    "* **Execute the Data Cleaning Plan**\n",
    "* **Manually correct any additional errors**\n",
    "\n",
    "### Data Exploration  \n",
    "* **Explore each feature**  \n",
    "   During this step, we will explore the values of each feature (e.g., distribution of continues variables, unique values of categorical variables. This step will be performed to a significant degree during Data Wrangling, but some operations may take place during this phase also.\n",
    "* **Explore correlations between the variables**  \n",
    "   Evaluating correlations between features may provide us with intuition about ways to simplify our model later in the process or reveal dominant features in the dataset.  \n",
    "   \n",
    "### Feature Engineering  \n",
    "In the Feature Engineering phase, we will use features from the dataset to create new more valuable features. More specifically, we will extract at least the time of the day from the \"Dates\" variable, but additional ideas for extraction may arise during the Exploration phase. This step usually involves the generation of polynomial features, but in our case, this technic does not seem appropriate.\n",
    "\n",
    "### Data Normalization\n",
    "During this step, the Normalization/Standardization of the numerical features will take place. This step is essential for some algorithms that are prone to bad prediction if the individual features do not more or less look like standard normally distributed data.  \n",
    "\n",
    "### Data Transformation  \n",
    "* **Encoding categorical features**  \n",
    "   Many of the algorithms that will be used do not accept categorical/string values; thus we need to encode them. We will evaluate the following encoding methodologies:\n",
    "    * One-hot Encoding\n",
    "    * Label Encoding\n",
    "    * Embeddings (for the Neural Networks implementation)  \n",
    "* **Discretization of numerical features**  \n",
    "   Although the coordinates variables are numerical, they cannot be used \"as is\". Using them directly would imply that the possibility of a specific type of crime increases towards a specific direction which is not the case. Instead, we will apply Discretization, to divide the given area to a grid of smaller areas. This way we may be able to spot correlations between specific areas and crime categories.  \n",
    "* **Transformation of cyclic ordinal features**\n",
    "   The time and day features that will be extracted from the \"Dates\" timestamp also cannot be used as is. The problem arises from the fact that although 0 is very far from 23 as numbers in reality 0:00 and 23:00 are just one hour apart (the same stands for the 1st and 7th day of the week). To overcome this, we will apply a trigonometric transformation with the use of `x_hour=sin(2pi*hour/24)`, `y_hour=cos(2pi*hour/24)` and `x_weekday=sin(2pi*weekday/7)`, `y_weekday=cos(2pi*weekday/24)`\n",
    "\n",
    "### Training / Testing data creation  \n",
    "The dataset will be split into training and testing data. Since the rows of in our dataset are significantly higher than the columns (878,000 vs. 9), most probably we will work with a default 80%-20% split but more methods may be tried (e.g., cross-validation). Note that since this is a dataset from a past Kaggle competition, there is no need to create a validation set; instead, submitting our predictions for the test set will serve as the validation process.  \n",
    "\n",
    "### Model selection and evaluation\n",
    "During the model selection, initially, we will try several algorithms will be evaluated with minimal or no tuning at all. This step will provide us with an indication of which algorithms work best. Based on the results we will select the model(s) that seems most appropriate, and we will proceed with Hyper-parameters Tuning.  \n",
    "The algorithms we are planning to use are the following:  \n",
    "* Stochastic Gradient Descent\n",
    "* Nearest Neighbors\n",
    "* Support Vector Machines\n",
    "* Forests of randomized trees\n",
    "* AdaBoost\n",
    "* Gradient Tree Boosting\n",
    "* Neural Networks  \n",
    "Some of the above algorithms do not scale well with big datasets (as an example Support Vector Machines scales exponentially with the number of rows). In such occasions, based on the training time, we may use samples of the training data.\n",
    "\n",
    "### References  \n",
    "Kaggle - [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime)  \n",
    "Rienks R. (2015) - [\"Predictive Policing: Taking a chance for a safer future\"](http://issuu.com/rutgerrienks/docs/predictive_policing_rienks_uk)  \n",
    "Walter L. Perry, Brian McInnis, Carter C. Price, Susan Smith, John S. Hollywood - [Predictive Policing - The Role of Crime Forecasting in Law Enforcement Operations](https://www.rand.org/content/dam/rand/pubs/research_reports/RR200/RR233/RAND_RR233.pdf)  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
